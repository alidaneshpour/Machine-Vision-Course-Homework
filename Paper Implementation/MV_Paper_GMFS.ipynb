{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7tGpN3KgjNv",
        "outputId": "6100f1b2-9ae0-46bd-9aae-095ea18aa214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All images are renamed and moved to /content/DBs_Dataset/original_images, and /content/fingerprint_datasets has been deleted.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# URLs for datasets\n",
        "urls = {\n",
        "    \"DB1\": \"http://bias.csr.unibo.it/fvc2004/Downloads/DB1_B.zip\",\n",
        "    \"DB2\": \"http://bias.csr.unibo.it/fvc2004/Downloads/DB2_B.zip\",\n",
        "    \"DB3\": \"http://bias.csr.unibo.it/fvc2004/Downloads/DB3_B.zip\",\n",
        "    \"DB4\": \"http://bias.csr.unibo.it/fvc2004/Downloads/DB4_B.zip\"\n",
        "}\n",
        "\n",
        "# Directory to save datasets\n",
        "base_dir = \"/content/fingerprint_datasets\"\n",
        "final_dir = \"/content/DBs_Dataset\"\n",
        "original_images_dir = os.path.join(final_dir, \"original_images\")\n",
        "\n",
        "# Create the necessary directories\n",
        "os.makedirs(original_images_dir, exist_ok=True)\n",
        "\n",
        "# Function to download and extract dataset\n",
        "def download_and_extract(url, save_dir):\n",
        "    # Create a folder for each database to store the extracted files separately\n",
        "    db_name = os.path.splitext(os.path.basename(url))[0]\n",
        "    db_save_dir = os.path.join(save_dir, db_name)\n",
        "    os.makedirs(db_save_dir, exist_ok=True)\n",
        "\n",
        "    zip_path = os.path.join(db_save_dir, os.path.basename(url))\n",
        "\n",
        "    if not os.path.exists(zip_path):\n",
        "        response = requests.get(url)\n",
        "        with open(zip_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "    # Extract the zip file to the corresponding directory\n",
        "    extract_dir = os.path.splitext(zip_path)[0]\n",
        "    if not os.path.exists(extract_dir):\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "    return extract_dir, db_name\n",
        "\n",
        "# Download and extract all datasets into separate directories\n",
        "extracted_dirs = {}\n",
        "for db, url in urls.items():\n",
        "    extracted_dirs[db] = download_and_extract(url, base_dir)\n",
        "\n",
        "# Move and rename all image files into the final folder with prefix\n",
        "for db, (db_dir, db_name) in extracted_dirs.items():\n",
        "    for root, dirs, files in os.walk(db_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.tif'):\n",
        "                # Construct the new file name with \"FVC2004_\" prefix\n",
        "                new_name = f\"FVC2004_{db_name}_{file}\"  # Add \"FVC2004_\" prefix here\n",
        "                old_path = os.path.join(root, file)\n",
        "                new_path = os.path.join(original_images_dir, new_name)\n",
        "\n",
        "                # Rename and move the file\n",
        "                shutil.move(old_path, new_path)\n",
        "\n",
        "# Delete the 'fingerprint_datasets' directory after the work is complete\n",
        "shutil.rmtree(base_dir)\n",
        "\n",
        "# Final output print\n",
        "print(f\"All images are renamed and moved to {original_images_dir}, and {base_dir} has been deleted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/raffaele-cappelli/pyfing.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGQtnexugsRk",
        "outputId": "8e2cf8c7-7b2b-412e-a1af-c6229c6c4a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pyfing'...\n",
            "remote: Enumerating objects: 196, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 196 (delta 24), reused 28 (delta 13), pack-reused 142 (from 1)\u001b[K\n",
            "Receiving objects: 100% (196/196), 104.90 MiB | 33.70 MiB/s, done.\n",
            "Resolving deltas: 100% (76/76), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the file\n",
        "file_path = '/content/pyfing/pyfing/segmentation.py'\n",
        "\n",
        "# Open the file in read mode to get its content\n",
        "with open(file_path, 'r') as file:\n",
        "    content = file.readlines()\n",
        "\n",
        "# Modify the specific line in the content\n",
        "modified_content = []\n",
        "for line in content:\n",
        "    if \"from .definitions import Image, Parameters\" in line:\n",
        "        modified_content.append(\"from definitions import Image, Parameters\\n\")\n",
        "    else:\n",
        "        modified_content.append(line)\n",
        "\n",
        "# Write the modified content back to the file\n",
        "with open(file_path, 'w') as file:\n",
        "    file.writelines(modified_content)\n",
        "\n",
        "print(f\"Modified the file {file_path} successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnMn0ttpgsmY",
        "outputId": "f68f2803-8974-41b6-c607-6c9f119b8643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified the file /content/pyfing/pyfing/segmentation.py successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "# URL to the dataset\n",
        "url = 'https://figshare.com/ndownloader/files/1871964'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Save the zip file to disk\n",
        "    zip_file_path = '/content/dataset_file.zip'\n",
        "    with open(zip_file_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(\"File downloaded successfully.\")\n",
        "\n",
        "    # Create the target folder if it doesn't exist\n",
        "    target_folder = '/content/DBs_Dataset/validation_images'\n",
        "    os.makedirs(target_folder, exist_ok=True)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        # Extract all files to the '/content/dataset' folder\n",
        "        zip_ref.extractall('/content/dataset')\n",
        "        print(\"Files extracted successfully.\")\n",
        "\n",
        "        # Define the correct path to the 'GroundTruth' folder\n",
        "        extracted_folder = '/content/dataset/SegmentationBenchmark/GroundTruth'\n",
        "\n",
        "        # Check if the 'GroundTruth' folder exists\n",
        "        if os.path.exists(extracted_folder):\n",
        "            # Loop through all the files in the 'GroundTruth' folder\n",
        "            for file_name in os.listdir(extracted_folder):\n",
        "                file_path = os.path.join(extracted_folder, file_name)\n",
        "\n",
        "                # Check if the file is an image (you can add more extensions as needed)\n",
        "                if file_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):\n",
        "                    # Updated regular expression to match filenames like FVC2000_DB1_IM_54_6seg.png\n",
        "                    match = re.match(r\"([A-Za-z0-9]+)_IM_(\\d+_\\d+)[^.]+(\\.png|\\.jpg|\\.jpeg|\\.bmp|\\.gif)$\", file_name)\n",
        "                    if match:\n",
        "                        # Extract the parts of the filename for renaming\n",
        "                        part1 = match.group(1)   # This captures the first part (e.g., FVC2000_DB1)\n",
        "                        part2 = match.group(2)   # This captures the second part (e.g., 54_6)\n",
        "\n",
        "                        # New file name with the '_B_' in place of 'IM' and '.tif' extension\n",
        "                        new_file_name = f\"{part1}_B_{part2}.tif\"\n",
        "                        new_file_path = os.path.join(target_folder, new_file_name)\n",
        "\n",
        "                        # Move and rename the file\n",
        "                        shutil.move(file_path, new_file_path)\n",
        "                        #print(f\"Renamed and moved: {file_name} to {new_file_name}\")\n",
        "                    else:\n",
        "                        # If the file doesn't match the expected pattern, move it without renaming\n",
        "                        shutil.move(file_path, os.path.join(target_folder, file_name))\n",
        "                        #print(f\"Moved: {file_name} without renaming.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzfSaDZ-hbnT",
        "outputId": "a61ab821-f002-4baf-e5e7-ce51013bcff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully.\n",
            "Files extracted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Directory containing the images\n",
        "input_directory = '/content/DBs_Dataset/validation_images/'\n",
        "\n",
        "# Iterate through all the files in the directory\n",
        "for filename in os.listdir(input_directory):\n",
        "    # Check if the file is a PNG image (you can change to match other formats as well)\n",
        "    if filename.endswith('.png'):\n",
        "        # Construct the full file path\n",
        "        file_path = os.path.join(input_directory, filename)\n",
        "\n",
        "        # Check if the file contains \"IM\" and rename it to \"B\"\n",
        "        if \"IM\" in filename:\n",
        "            new_filename = filename.replace(\"IM\", \"B\")\n",
        "            new_filename = new_filename.replace(\".png\", \".tif\")  # Change extension to .tif\n",
        "\n",
        "            # Construct the new file path\n",
        "            new_file_path = os.path.join(input_directory, new_filename)\n",
        "\n",
        "            # Open the image using PIL\n",
        "            with Image.open(file_path) as img:\n",
        "                # Convert image to grayscale\n",
        "                img_gray = img.convert('L')\n",
        "\n",
        "                # Apply a binary threshold (you can adjust the threshold value)\n",
        "                threshold = 128  # This can be adjusted\n",
        "                img_binary = img_gray.point(lambda p: p > threshold and 255)\n",
        "\n",
        "                # Save the binary image in TIFF format\n",
        "                img_binary.save(new_file_path, format='TIFF')\n",
        "\n",
        "            # Optionally, delete the original image (uncomment if you want to remove it)\n",
        "            # os.remove(file_path)\n",
        "\n",
        "            # Print the renaming action\n",
        "            #print(f\"Renamed and converted {filename} to {new_filename}\")\n",
        "\n",
        "shutil.rmtree('/content/dataset')\n",
        "\n",
        "import os\n",
        "\n",
        "# Specify the file path\n",
        "file_path = '/content/dataset_file.zip'\n",
        "\n",
        "# Remove the file\n",
        "os.remove(file_path)\n",
        "\n",
        "# Confirm deletion\n",
        "print(f\"{file_path} has been deleted.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Mbpi9Vxhgd6",
        "outputId": "0d7135d9-fec5-41c4-d742-37d888edf7c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset_file.zip has been deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/pyfing/pyfing')\n",
        "\n",
        "# Now you should be able to import the segmentation module\n",
        "import segmentation\n",
        "from segmentation import Gmfs, GmfsParameters\n",
        "\n",
        "# Directories\n",
        "input_dir = '/content/DBs_Dataset/original_images'\n",
        "output_dir = '/content/DBs_Dataset/segmented_images'\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Loop through all .tif images in the original_images directory\n",
        "for image_path in glob(os.path.join(input_dir, '*.tif')):\n",
        "    # Read the image in grayscale\n",
        "    image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Check if the image was loaded successfully\n",
        "    if image is not None:\n",
        "        # Initialize the GmfsParameters and Gmfs objects\n",
        "        parameters = GmfsParameters()\n",
        "        segmentation_method = Gmfs(parameters)\n",
        "\n",
        "        # Perform the segmentation\n",
        "        segmented_image = segmentation_method.run(image)\n",
        "\n",
        "        # Construct the output path with the original name and \"_seg\" suffix\n",
        "        filename = os.path.basename(image_path)\n",
        "        output_path = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_paperseg.tif\")\n",
        "\n",
        "        # Save the segmented image\n",
        "        cv.imwrite(output_path, segmented_image)\n",
        "\n",
        "# Final output print\n",
        "print(f\"Segmentation completed. Segmented images are saved in {output_dir}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWx1gQ5Sgu0q",
        "outputId": "dfdbbf1f-4dd9-47b3-fdc0-1b5121cefbcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation completed. Segmented images are saved in /content/DBs_Dataset/segmented_images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Path to the dataset folder\n",
        "dataset_folder = '/content/DBs_Dataset/segmented_images'\n",
        "\n",
        "# Function to check the center of an image and process accordingly\n",
        "def process_image(image_path):\n",
        "    # Load the image in grayscale mode\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if img is None:\n",
        "        print(f\"Error loading image {image_path}\")\n",
        "        return\n",
        "\n",
        "    # Get the center pixel coordinates\n",
        "    height, width = img.shape\n",
        "    center = (height // 2, width // 2)\n",
        "\n",
        "    # Get the center pixel value (0 for black, 255 for white in grayscale)\n",
        "    center_pixel = img[center]\n",
        "\n",
        "    if center_pixel == 255:  # Black center, invert the image\n",
        "        inverted_img = cv2.bitwise_not(img)\n",
        "        # Overwrite the original image with the inverted image\n",
        "        cv2.imwrite(image_path, inverted_img)\n",
        "\n",
        "\n",
        "# Iterate through all images in the dataset folder\n",
        "for filename in os.listdir(dataset_folder):\n",
        "    image_path = os.path.join(dataset_folder, filename)\n",
        "\n",
        "    # Check if it's an image file (you can add more formats if needed)\n",
        "    if filename.endswith(('.jpg', '.png', '.tif')):\n",
        "        process_image(image_path)\n"
      ],
      "metadata": {
        "id": "QSVoWNgniDvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# Pre-processing and error calculation\n",
        "def calculate_error_probabilities(seg_image_path, val_image_path):\n",
        "    # Load the segmented image and the validation image (ground truth)\n",
        "    seg_image = cv2.imread(seg_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    val_image = cv2.imread(val_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if seg_image is None or val_image is None:\n",
        "        print(f\"Error: Could not load images {seg_image_path} or {val_image_path}\")\n",
        "        return None, None, None  # Return None if there is an issue loading images\n",
        "\n",
        "    # Ensure the images are binary (foreground=255, background=0)\n",
        "    _, seg_image = cv2.threshold(seg_image, 128, 255, cv2.THRESH_BINARY)\n",
        "    _, val_image = cv2.threshold(val_image, 128, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Calculate True Background (Nbrb) and True Foreground (Nbrf)\n",
        "    Nbrb = np.sum(val_image == 0)  # True background pixels in validation\n",
        "    Nbrf = np.sum(val_image == 255)  # True foreground pixels in validation\n",
        "\n",
        "    # Calculate Number of Background Classification Errors (Nbrbe)\n",
        "    Nbrbe = np.sum((seg_image == 255) & (val_image == 0))  # Foreground classified as background\n",
        "\n",
        "    # Calculate Number of Foreground Classification Errors (Nbrfe)\n",
        "    Nbrfe = np.sum((seg_image == 0) & (val_image == 255))  # Background classified as foreground\n",
        "\n",
        "    # Calculate Prob1 and Prob2\n",
        "    Prob1 = Nbrbe / Nbrb if Nbrb > 0 else 0  # Probability that foreground is classified as background\n",
        "    Prob2 = Nbrfe / Nbrf if Nbrf > 0 else 0  # Probability that background is classified as foreground\n",
        "\n",
        "    # Calculate ProbErr\n",
        "    ProbErr = (Prob1 + Prob2) / 2\n",
        "\n",
        "    return Prob1, Prob2, ProbErr\n",
        "\n",
        "# Directories\n",
        "segmentation_dir = '/content/DBs_Dataset/segmented_images'\n",
        "validation_dir = '/content/DBs_Dataset/validation_images'\n",
        "\n",
        "# List of image prefixes for DB1, DB2, DB3, DB4\n",
        "datasets = ['DB1', 'DB2', 'DB3', 'DB4']\n",
        "\n",
        "# Initialize a dictionary to store the results\n",
        "final_results = {}\n",
        "\n",
        "# Calculate the error probabilities for each image in the segmentation folder\n",
        "for dataset in datasets:\n",
        "    # Store the results for the current dataset\n",
        "    Prob1_values = []\n",
        "    Prob2_values = []\n",
        "    ProbErr_values = []\n",
        "\n",
        "    # Loop through each image in the dataset's segmentation folder\n",
        "    for image_path in glob(os.path.join(segmentation_dir, f'FVC2004_{dataset}_*.tif')):\n",
        "        # Construct the corresponding validation image path\n",
        "        val_image_name = os.path.basename(image_path).replace('_paperseg.tif', 'seg.tif')\n",
        "        val_image_path = os.path.join(validation_dir, val_image_name)\n",
        "\n",
        "        # Calculate the error probabilities\n",
        "        Prob1, Prob2, ProbErr = calculate_error_probabilities(image_path, val_image_path)\n",
        "\n",
        "        if Prob1 is not None:\n",
        "            # Store the values in the corresponding lists\n",
        "            Prob1_values.append(Prob1)\n",
        "            Prob2_values.append(Prob2)\n",
        "            ProbErr_values.append(ProbErr)\n",
        "\n",
        "    # Calculate the average for each probability\n",
        "    if Prob1_values:\n",
        "        avg_Prob1 = np.mean(Prob1_values)\n",
        "        avg_Prob2 = np.mean(Prob2_values)\n",
        "        avg_ProbErr = np.mean(ProbErr_values)\n",
        "\n",
        "        # Store the averages for the dataset\n",
        "        final_results[dataset] = {\n",
        "            'avg_Prob1': avg_Prob1,\n",
        "            'avg_Prob2': avg_Prob2,\n",
        "            'avg_ProbErr': avg_ProbErr\n",
        "        }\n",
        "\n",
        "# Print the final average results for each dataset\n",
        "for dataset, metrics in final_results.items():\n",
        "    print(f\"Average Results for {dataset}:\")\n",
        "    print(f\"avg_Prob1: {metrics['avg_Prob1']:.4f}, avg_Prob2: {metrics['avg_Prob2']:.4f}, avg_ProbErr: {metrics['avg_ProbErr']:.4f}\")\n",
        "\n",
        "# Calculate and print overall averages across all datasets\n",
        "all_Prob1 = [metrics['avg_Prob1'] for metrics in final_results.values()]\n",
        "all_Prob2 = [metrics['avg_Prob2'] for metrics in final_results.values()]\n",
        "all_ProbErr = [metrics['avg_ProbErr'] for metrics in final_results.values()]\n",
        "\n",
        "print(\"\\nOverall Average Results:\")\n",
        "print(f\"Overall avg_Prob1: {np.mean(all_Prob1):.4f}, Overall avg_Prob2: {np.mean(all_Prob2):.4f}, Overall avg_ProbErr: {np.mean(all_ProbErr):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImiMdlKhg0q1",
        "outputId": "b2b1c50f-f2c2-4f47-ede3-47dad5e5fe0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Results for DB1:\n",
            "avg_Prob1: 0.1054, avg_Prob2: 0.0147, avg_ProbErr: 0.0601\n",
            "Average Results for DB2:\n",
            "avg_Prob1: 0.0596, avg_Prob2: 0.0507, avg_ProbErr: 0.0552\n",
            "Average Results for DB3:\n",
            "avg_Prob1: 0.0611, avg_Prob2: 0.0066, avg_ProbErr: 0.0338\n",
            "Average Results for DB4:\n",
            "avg_Prob1: 0.0662, avg_Prob2: 0.0023, avg_ProbErr: 0.0342\n",
            "\n",
            "Overall Average Results:\n",
            "Overall avg_Prob1: 0.0731, Overall avg_Prob2: 0.0186, Overall avg_ProbErr: 0.0458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86Q5vEZCh8AU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}