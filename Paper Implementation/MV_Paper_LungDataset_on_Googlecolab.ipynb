{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4LKRvoov4pk",
        "outputId": "e73e43d9-56cc-4c5a-cda7-e654b890bf39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1ffbbyoPf-I3Y0iGbBahXpWqYdGd7xxQQ\n",
            "From (redirected): https://drive.google.com/uc?id=1ffbbyoPf-I3Y0iGbBahXpWqYdGd7xxQQ&confirm=t&uuid=849c57c9-1da6-489b-9481-ac2340ae3ee5\n",
            "To: /content/dataset.tar.gz\n",
            "100%|██████████| 4.40G/4.40G [01:06<00:00, 65.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n",
            "['.config', 'dataset.tar.gz', 'dataset', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import gdown\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "# Step 1: Download the .tar.gz file from Google Drive\n",
        "file_id = '1ffbbyoPf-I3Y0iGbBahXpWqYdGd7xxQQ'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output_path = 'dataset.tar.gz'\n",
        "gdown.download(url, output_path, quiet=False)\n",
        "\n",
        "# Step 2: Extract the .tar.gz file\n",
        "with tarfile.open(output_path, 'r:gz') as tar:\n",
        "    tar.extractall()\n",
        "\n",
        "print(\"Extraction complete.\")\n",
        "\n",
        "# Step 3: Check extracted files\n",
        "extracted_files = os.listdir()\n",
        "print(extracted_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Define paths\n",
        "input_folder = '/content/dataset/images'\n",
        "output_folder = '/content/dataset/segmented'\n",
        "\n",
        "# Ensure the output folder exists\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Block size for processing\n",
        "block_size = (16, 16)\n",
        "\n",
        "# Contour smoothing function\n",
        "def smooth_contours(mask, cutoff=5):\n",
        "    mask = (mask > 0).astype(np.uint8)\n",
        "    f_transform = np.fft.fft2(mask)\n",
        "    f_shift = np.fft.fftshift(f_transform)\n",
        "    rows, cols = mask.shape\n",
        "    crow, ccol = rows // 2, cols // 2\n",
        "    mask_filter = np.zeros((rows, cols), np.uint8)\n",
        "    cv2.circle(mask_filter, (ccol, crow), cutoff, 1, -1)\n",
        "    f_shift_filtered = f_shift * mask_filter\n",
        "    f_ishift = np.fft.ifftshift(f_shift_filtered)\n",
        "    smoothed = np.fft.ifft2(f_ishift)\n",
        "    smoothed = np.abs(smoothed)\n",
        "    smoothed = (smoothed > 0.5).astype(np.uint8) * 255\n",
        "    return smoothed\n",
        "\n",
        "# Feature extraction function\n",
        "def extract_features(block, global_mean):\n",
        "    variance = np.var(block)\n",
        "    local_mean = np.mean(block)\n",
        "    diff_mean = local_mean - global_mean\n",
        "    sobelx = cv2.Sobel(block, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobely = cv2.Sobel(block, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    gx2 = np.mean(sobelx**2)\n",
        "    gy2 = np.mean(sobely**2)\n",
        "    gxy = np.mean(sobelx * sobely)\n",
        "    coherence = np.sqrt((gx2 - gy2)**2 + 4 * gxy**2) / (gx2 + gy2 + 1e-8)\n",
        "    ridge_direction = 0.5 * np.arctan2(2 * gxy, gx2 - gy2)\n",
        "    fft = np.fft.fft2(block)\n",
        "    energy_spectrum = np.sqrt(np.sum(np.real(fft)**2 + np.imag(fft)**2))\n",
        "    return [variance, diff_mean, coherence, ridge_direction, energy_spectrum]\n",
        "\n",
        "# Process each image in the input folder\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.lower().endswith(('.tif', '.png', '.jpg', '.jpeg', '.bmp')):\n",
        "        input_path = os.path.join(input_folder, filename)\n",
        "        output_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}_paperseg.tif\")\n",
        "\n",
        "        # Load image in grayscale\n",
        "        image = cv2.imread(input_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if image is None:\n",
        "            print(f\"Error: Could not load image {input_path}\")\n",
        "            continue\n",
        "\n",
        "        # Calculate global mean\n",
        "        global_mean = np.mean(image)\n",
        "\n",
        "        # Apply Sobel filter\n",
        "        sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "        sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "        sobel_magnitude = cv2.magnitude(sobel_x, sobel_y)\n",
        "        sobel_magnitude = np.uint8(np.abs(sobel_magnitude))\n",
        "\n",
        "        # Morphological operations\n",
        "        kernel = np.ones((3, 3), np.uint8)\n",
        "        opened_image = cv2.morphologyEx(sobel_magnitude, cv2.MORPH_OPEN, kernel)\n",
        "        top_hat_opened_image = cv2.subtract(sobel_magnitude, opened_image)\n",
        "\n",
        "        # Extract features and prepare for clustering\n",
        "        h, w = top_hat_opened_image.shape\n",
        "        feature_vectors = []\n",
        "        coordinates = []\n",
        "\n",
        "        for y in range(0, h, block_size[0]):\n",
        "            for x in range(0, w, block_size[1]):\n",
        "                block = top_hat_opened_image[y:y + block_size[0], x:x + block_size[1]]\n",
        "                features = extract_features(block, global_mean)\n",
        "                feature_vectors.append(features)\n",
        "                coordinates.append((y, x))\n",
        "\n",
        "        feature_vectors = np.array(feature_vectors)\n",
        "\n",
        "        # Apply K-Means clustering\n",
        "        kmeans = KMeans(n_clusters=2, random_state=0).fit(feature_vectors)\n",
        "        labels_kmeans = kmeans.labels_\n",
        "\n",
        "        # Map labels back to image\n",
        "        clustered_image_kmeans = np.zeros((h, w), dtype=np.uint8)\n",
        "        for idx, (y, x) in enumerate(coordinates):\n",
        "            clustered_image_kmeans[y:y + block_size[0], x:x + block_size[1]] = labels_kmeans[idx] * 255\n",
        "\n",
        "        # Smooth contours\n",
        "        smoothed_contours_image = smooth_contours(clustered_image_kmeans)\n",
        "\n",
        "        # Save the output image\n",
        "        cv2.imwrite(output_path, smoothed_contours_image)\n",
        "        #print(f\"Processed and saved: {output_path}\")"
      ],
      "metadata": {
        "id": "2VyIoKpd1dZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# Pre-processing and error calculation\n",
        "def calculate_error_probabilities(seg_image_path, val_image_path):\n",
        "    # Load the segmented image and the validation image (ground truth)\n",
        "    seg_image = cv2.imread(seg_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    val_image = cv2.imread(val_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if seg_image is None or val_image is None:\n",
        "        print(f\"Error: Could not load images {seg_image_path} or {val_image_path}\")\n",
        "        return None, None, None  # Return None if there is an issue loading images\n",
        "\n",
        "    # Ensure the images are binary (foreground=255, background=0)\n",
        "    _, seg_image = cv2.threshold(seg_image, 128, 255, cv2.THRESH_BINARY)\n",
        "    _, val_image = cv2.threshold(val_image, 128, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Calculate True Background (Nbrb) and True Foreground (Nbrf)\n",
        "    Nbrb = np.sum(val_image == 0)  # True background pixels in validation\n",
        "    Nbrf = np.sum(val_image == 255)  # True foreground pixels in validation\n",
        "\n",
        "    # Calculate Number of Background Classification Errors (Nbrbe)\n",
        "    Nbrbe = np.sum((seg_image == 255) & (val_image == 0))  # Foreground classified as background\n",
        "\n",
        "    # Calculate Number of Foreground Classification Errors (Nbrfe)\n",
        "    Nbrfe = np.sum((seg_image == 0) & (val_image == 255))  # Background classified as foreground\n",
        "\n",
        "    # Calculate Prob1 and Prob2\n",
        "    Prob1 = Nbrbe / Nbrb if Nbrb > 0 else 0  # Probability that foreground is classified as background\n",
        "    Prob2 = Nbrfe / Nbrf if Nbrf > 0 else 0  # Probability that background is classified as foreground\n",
        "\n",
        "    # Calculate ProbErr\n",
        "    ProbErr = (Prob1 + Prob2) / 2\n",
        "\n",
        "    return Prob1, Prob2, ProbErr\n",
        "\n",
        "# Directories\n",
        "segmentation_dir = '/content/dataset/segmented'\n",
        "validation_dir = '/content/dataset/masks'\n",
        "\n",
        "# Initialize a dictionary to store the results\n",
        "final_results = {}\n",
        "\n",
        "# Calculate the error probabilities for each image in the segmentation folder\n",
        "Prob1_values = []\n",
        "Prob2_values = []\n",
        "ProbErr_values = []\n",
        "\n",
        "# Loop through each segmented image in the segmentation directory\n",
        "for seg_image_path in glob(os.path.join(segmentation_dir, '*paperseg.tif')):\n",
        "    # Construct the corresponding validation image path\n",
        "    filename = os.path.basename(seg_image_path)\n",
        "    val_image_name = filename.replace('paperseg.tif', 'mask.png')\n",
        "    val_image_path = os.path.join(validation_dir, val_image_name)\n",
        "\n",
        "    # Calculate the error probabilities\n",
        "    Prob1, Prob2, ProbErr = calculate_error_probabilities(seg_image_path, val_image_path)\n",
        "\n",
        "    if Prob1 is not None:\n",
        "        # Store the values in the corresponding lists\n",
        "        Prob1_values.append(Prob1)\n",
        "        Prob2_values.append(Prob2)\n",
        "        ProbErr_values.append(ProbErr)\n",
        "\n",
        "# Calculate the average for each probability\n",
        "if Prob1_values:\n",
        "    avg_Prob1 = np.mean(Prob1_values)\n",
        "    avg_Prob2 = np.mean(Prob2_values)\n",
        "    avg_ProbErr = np.mean(ProbErr_values)\n",
        "\n",
        "    # Store the averages\n",
        "    final_results = {\n",
        "        'avg_Prob1': avg_Prob1,\n",
        "        'avg_Prob2': avg_Prob2,\n",
        "        'avg_ProbErr': avg_ProbErr\n",
        "    }\n",
        "\n",
        "# Print the final average results\n",
        "print(\"Average Results:\")\n",
        "print(f\"avg_Prob1: {final_results['avg_Prob1']:.4f}, avg_Prob2: {final_results['avg_Prob2']:.4f}, avg_ProbErr: {final_results['avg_ProbErr']:.4f}\")\n",
        "\n",
        "# Calculate and print overall averages across all datasets (if you had multiple datasets)\n",
        "all_Prob1 = [final_results['avg_Prob1']]\n",
        "all_Prob2 = [final_results['avg_Prob2']]\n",
        "all_ProbErr = [final_results['avg_ProbErr']]\n",
        "\n",
        "print(\"\\nOverall Average Results:\")\n",
        "print(f\"Overall avg_Prob1: {np.mean(all_Prob1):.4f}, Overall avg_Prob2: {np.mean(all_Prob2):.4f}, Overall avg_ProbErr: {np.mean(all_ProbErr):.4f}\")\n"
      ],
      "metadata": {
        "id": "PDNINny22-3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lhLXkrfynsRX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}