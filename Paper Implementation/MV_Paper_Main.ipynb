{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLR_pP7ks1UY",
        "outputId": "bef4b462-642b-4e23-df8f-4af2416e6a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All images are renamed and moved to /content/DBs_Dataset/original_images, and /content/fingerprint_datasets has been deleted.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# URLs for datasets\n",
        "urls = {\n",
        "    \"DB1\": \"http://bias.csr.unibo.it/fvc2004/Downloads/DB1_B.zip\",\n",
        "    \"DB2\": \"http://bias.csr.unibo.it/fvc2004/Downloads/DB2_B.zip\",\n",
        "    \"DB3\": \"http://bias.csr.unibo.it/fvc2004/Downloads/DB3_B.zip\",\n",
        "    \"DB4\": \"http://bias.csr.unibo.it/fvc2004/Downloads/DB4_B.zip\"\n",
        "}\n",
        "\n",
        "# Directory to save datasets\n",
        "base_dir = \"/content/fingerprint_datasets\"\n",
        "final_dir = \"/content/DBs_Dataset\"\n",
        "original_images_dir = os.path.join(final_dir, \"original_images\")\n",
        "\n",
        "# Create the necessary directories\n",
        "os.makedirs(original_images_dir, exist_ok=True)\n",
        "\n",
        "# Function to download and extract dataset\n",
        "def download_and_extract(url, save_dir):\n",
        "    # Create a folder for each database to store the extracted files separately\n",
        "    db_name = os.path.splitext(os.path.basename(url))[0]\n",
        "    db_save_dir = os.path.join(save_dir, db_name)\n",
        "    os.makedirs(db_save_dir, exist_ok=True)\n",
        "\n",
        "    zip_path = os.path.join(db_save_dir, os.path.basename(url))\n",
        "\n",
        "    if not os.path.exists(zip_path):\n",
        "        response = requests.get(url)\n",
        "        with open(zip_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "    # Extract the zip file to the corresponding directory\n",
        "    extract_dir = os.path.splitext(zip_path)[0]\n",
        "    if not os.path.exists(extract_dir):\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "    return extract_dir, db_name\n",
        "\n",
        "# Download and extract all datasets into separate directories\n",
        "extracted_dirs = {}\n",
        "for db, url in urls.items():\n",
        "    extracted_dirs[db] = download_and_extract(url, base_dir)\n",
        "\n",
        "# Move and rename all image files into the final folder with prefix\n",
        "for db, (db_dir, db_name) in extracted_dirs.items():\n",
        "    for root, dirs, files in os.walk(db_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.tif'):\n",
        "                # Construct the new file name with \"FVC2004_\" prefix\n",
        "                new_name = f\"FVC2004_{db_name}_{file}\"  # Add \"FVC2004_\" prefix here\n",
        "                old_path = os.path.join(root, file)\n",
        "                new_path = os.path.join(original_images_dir, new_name)\n",
        "\n",
        "                # Rename and move the file\n",
        "                shutil.move(old_path, new_path)\n",
        "\n",
        "# Delete the 'fingerprint_datasets' directory after the work is complete\n",
        "shutil.rmtree(base_dir)\n",
        "\n",
        "# Final output print\n",
        "print(f\"All images are renamed and moved to {original_images_dir}, and {base_dir} has been deleted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "# URL to the dataset\n",
        "url = 'https://figshare.com/ndownloader/files/1871964'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Save the zip file to disk\n",
        "    zip_file_path = '/content/dataset_file.zip'\n",
        "    with open(zip_file_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(\"File downloaded successfully.\")\n",
        "\n",
        "    # Create the target folder if it doesn't exist\n",
        "    target_folder = '/content/DBs_Dataset/validation_images'\n",
        "    os.makedirs(target_folder, exist_ok=True)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        # Extract all files to the '/content/dataset' folder\n",
        "        zip_ref.extractall('/content/dataset')\n",
        "        print(\"Files extracted successfully.\")\n",
        "\n",
        "        # Define the correct path to the 'GroundTruth' folder\n",
        "        extracted_folder = '/content/dataset/SegmentationBenchmark/GroundTruth'\n",
        "\n",
        "        # Check if the 'GroundTruth' folder exists\n",
        "        if os.path.exists(extracted_folder):\n",
        "            # Loop through all the files in the 'GroundTruth' folder\n",
        "            for file_name in os.listdir(extracted_folder):\n",
        "                file_path = os.path.join(extracted_folder, file_name)\n",
        "\n",
        "                # Check if the file is an image (you can add more extensions as needed)\n",
        "                if file_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):\n",
        "                    # Updated regular expression to match filenames like FVC2000_DB1_IM_54_6seg.png\n",
        "                    match = re.match(r\"([A-Za-z0-9]+)_IM_(\\d+_\\d+)[^.]+(\\.png|\\.jpg|\\.jpeg|\\.bmp|\\.gif)$\", file_name)\n",
        "                    if match:\n",
        "                        # Extract the parts of the filename for renaming\n",
        "                        part1 = match.group(1)   # This captures the first part (e.g., FVC2000_DB1)\n",
        "                        part2 = match.group(2)   # This captures the second part (e.g., 54_6)\n",
        "\n",
        "                        # New file name with the '_B_' in place of 'IM' and '.tif' extension\n",
        "                        new_file_name = f\"{part1}_B_{part2}.tif\"\n",
        "                        new_file_path = os.path.join(target_folder, new_file_name)\n",
        "\n",
        "                        # Move and rename the file\n",
        "                        shutil.move(file_path, new_file_path)\n",
        "                        #print(f\"Renamed and moved: {file_name} to {new_file_name}\")\n",
        "                    else:\n",
        "                        # If the file doesn't match the expected pattern, move it without renaming\n",
        "                        shutil.move(file_path, os.path.join(target_folder, file_name))\n",
        "                        #print(f\"Moved: {file_name} without renaming.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7Gwu_7ItrN4",
        "outputId": "3f1d543b-1d9c-40b1-de14-0fb939c051af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully.\n",
            "Files extracted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Directory containing the images\n",
        "input_directory = '/content/DBs_Dataset/validation_images/'\n",
        "\n",
        "# Iterate through all the files in the directory\n",
        "for filename in os.listdir(input_directory):\n",
        "    # Check if the file is a PNG image (you can change to match other formats as well)\n",
        "    if filename.endswith('.png'):\n",
        "        # Construct the full file path\n",
        "        file_path = os.path.join(input_directory, filename)\n",
        "\n",
        "        # Check if the file contains \"IM\" and rename it to \"B\"\n",
        "        if \"IM\" in filename:\n",
        "            new_filename = filename.replace(\"IM\", \"B\")\n",
        "            new_filename = new_filename.replace(\".png\", \".tif\")  # Change extension to .tif\n",
        "\n",
        "            # Construct the new file path\n",
        "            new_file_path = os.path.join(input_directory, new_filename)\n",
        "\n",
        "            # Open the image using PIL\n",
        "            with Image.open(file_path) as img:\n",
        "                # Convert image to grayscale\n",
        "                img_gray = img.convert('L')\n",
        "\n",
        "                # Apply a binary threshold (you can adjust the threshold value)\n",
        "                threshold = 128  # This can be adjusted\n",
        "                img_binary = img_gray.point(lambda p: p > threshold and 255)\n",
        "\n",
        "                # Save the binary image in TIFF format\n",
        "                img_binary.save(new_file_path, format='TIFF')\n",
        "\n",
        "            # Optionally, delete the original image (uncomment if you want to remove it)\n",
        "            # os.remove(file_path)\n",
        "\n",
        "            # Print the renaming action\n",
        "            #print(f\"Renamed and converted {filename} to {new_filename}\")\n",
        "\n",
        "shutil.rmtree('/content/dataset')\n",
        "\n",
        "import os\n",
        "\n",
        "# Specify the file path\n",
        "file_path = '/content/dataset_file.zip'\n",
        "\n",
        "# Remove the file\n",
        "os.remove(file_path)\n",
        "\n",
        "# Confirm deletion\n",
        "print(f\"{file_path} has been deleted.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIXROB8J4Kn1",
        "outputId": "2453a6f5-ab61-463f-dce8-3e1f5c0135d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset_file.zip has been deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Define paths\n",
        "input_folder = '/content/DBs_Dataset/original_images'\n",
        "output_folder = '/content/DBs_Dataset/segmented_images'\n",
        "\n",
        "# Ensure the output folder exists\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Block size for processing\n",
        "block_size = (16, 16)\n",
        "\n",
        "# Contour smoothing function\n",
        "def smooth_contours(mask, cutoff=5):\n",
        "    mask = (mask > 0).astype(np.uint8)\n",
        "    f_transform = np.fft.fft2(mask)\n",
        "    f_shift = np.fft.fftshift(f_transform)\n",
        "    rows, cols = mask.shape\n",
        "    crow, ccol = rows // 2, cols // 2\n",
        "    mask_filter = np.zeros((rows, cols), np.uint8)\n",
        "    cv2.circle(mask_filter, (ccol, crow), cutoff, 1, -1)\n",
        "    f_shift_filtered = f_shift * mask_filter\n",
        "    f_ishift = np.fft.ifftshift(f_shift_filtered)\n",
        "    smoothed = np.fft.ifft2(f_ishift)\n",
        "    smoothed = np.abs(smoothed)\n",
        "    smoothed = (smoothed > 0.5).astype(np.uint8) * 255\n",
        "    return smoothed\n",
        "\n",
        "# Feature extraction function\n",
        "def extract_features(block, global_mean):\n",
        "    variance = np.var(block)\n",
        "    local_mean = np.mean(block)\n",
        "    diff_mean = local_mean - global_mean\n",
        "    sobelx = cv2.Sobel(block, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobely = cv2.Sobel(block, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    gx2 = np.mean(sobelx**2)\n",
        "    gy2 = np.mean(sobely**2)\n",
        "    gxy = np.mean(sobelx * sobely)\n",
        "    coherence = np.sqrt((gx2 - gy2)**2 + 4 * gxy**2) / (gx2 + gy2 + 1e-8)\n",
        "    ridge_direction = 0.5 * np.arctan2(2 * gxy, gx2 - gy2)\n",
        "    fft = np.fft.fft2(block)\n",
        "    energy_spectrum = np.sqrt(np.sum(np.real(fft)**2 + np.imag(fft)**2))\n",
        "    return [variance, diff_mean, coherence, ridge_direction, energy_spectrum]\n",
        "\n",
        "# Process each image in the input folder\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.lower().endswith(('.tif', '.png', '.jpg', '.jpeg', '.bmp')):\n",
        "        input_path = os.path.join(input_folder, filename)\n",
        "        output_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}_paperseg.tif\")\n",
        "\n",
        "        # Load image in grayscale\n",
        "        image = cv2.imread(input_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if image is None:\n",
        "            print(f\"Error: Could not load image {input_path}\")\n",
        "            continue\n",
        "\n",
        "        # Calculate global mean\n",
        "        global_mean = np.mean(image)\n",
        "\n",
        "        # Apply Sobel filter\n",
        "        sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "        sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "        sobel_magnitude = cv2.magnitude(sobel_x, sobel_y)\n",
        "        sobel_magnitude = np.uint8(np.abs(sobel_magnitude))\n",
        "\n",
        "        # Morphological operations\n",
        "        kernel = np.ones((3, 3), np.uint8)\n",
        "        opened_image = cv2.morphologyEx(sobel_magnitude, cv2.MORPH_OPEN, kernel)\n",
        "        top_hat_opened_image = cv2.subtract(sobel_magnitude, opened_image)\n",
        "\n",
        "        # Extract features and prepare for clustering\n",
        "        h, w = top_hat_opened_image.shape\n",
        "        feature_vectors = []\n",
        "        coordinates = []\n",
        "\n",
        "        for y in range(0, h, block_size[0]):\n",
        "            for x in range(0, w, block_size[1]):\n",
        "                block = top_hat_opened_image[y:y + block_size[0], x:x + block_size[1]]\n",
        "                features = extract_features(block, global_mean)\n",
        "                feature_vectors.append(features)\n",
        "                coordinates.append((y, x))\n",
        "\n",
        "        feature_vectors = np.array(feature_vectors)\n",
        "\n",
        "        # Apply K-Means clustering\n",
        "        kmeans = KMeans(n_clusters=2, random_state=0).fit(feature_vectors)\n",
        "        labels_kmeans = kmeans.labels_\n",
        "\n",
        "        # Map labels back to image\n",
        "        clustered_image_kmeans = np.zeros((h, w), dtype=np.uint8)\n",
        "        for idx, (y, x) in enumerate(coordinates):\n",
        "            clustered_image_kmeans[y:y + block_size[0], x:x + block_size[1]] = labels_kmeans[idx] * 255\n",
        "\n",
        "        # Smooth contours\n",
        "        smoothed_contours_image = smooth_contours(clustered_image_kmeans)\n",
        "\n",
        "        # Save the output image\n",
        "        cv2.imwrite(output_path, smoothed_contours_image)\n",
        "        #print(f\"Processed and saved: {output_path}\")"
      ],
      "metadata": {
        "id": "msIMUvUu4-kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Path to the dataset folder\n",
        "dataset_folder = '/content/DBs_Dataset/segmented_images'\n",
        "\n",
        "# Function to check the center of an image and process accordingly\n",
        "def process_image(image_path):\n",
        "    # Load the image in grayscale mode\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if img is None:\n",
        "        print(f\"Error loading image {image_path}\")\n",
        "        return\n",
        "\n",
        "    # Get the center pixel coordinates\n",
        "    height, width = img.shape\n",
        "    center = (height // 2, width // 2)\n",
        "\n",
        "    # Get the center pixel value (0 for black, 255 for white in grayscale)\n",
        "    center_pixel = img[center]\n",
        "\n",
        "    if center_pixel == 255:  # white center, invert the image\n",
        "        inverted_img = cv2.bitwise_not(img)\n",
        "        # Overwrite the original image with the inverted image\n",
        "        cv2.imwrite(image_path, inverted_img)\n",
        "\n",
        "\n",
        "# Iterate through all images in the dataset folder\n",
        "for filename in os.listdir(dataset_folder):\n",
        "    image_path = os.path.join(dataset_folder, filename)\n",
        "\n",
        "    # Check if it's an image file (you can add more formats if needed)\n",
        "    if filename.endswith(('.jpg', '.png', '.tif')):\n",
        "        process_image(image_path)\n"
      ],
      "metadata": {
        "id": "xKMlaARmI5lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# Pre-processing and error calculation\n",
        "def calculate_error_probabilities(seg_image_path, val_image_path):\n",
        "    # Load the segmented image and the validation image (ground truth)\n",
        "    seg_image = cv2.imread(seg_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    val_image = cv2.imread(val_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if seg_image is None or val_image is None:\n",
        "        print(f\"Error: Could not load images {seg_image_path} or {val_image_path}\")\n",
        "        return None, None, None  # Return None if there is an issue loading images\n",
        "\n",
        "    # Ensure the images are binary (foreground=255, background=0)\n",
        "    _, seg_image = cv2.threshold(seg_image, 128, 255, cv2.THRESH_BINARY)\n",
        "    _, val_image = cv2.threshold(val_image, 128, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Calculate True Background (Nbrb) and True Foreground (Nbrf)\n",
        "    Nbrb = np.sum(val_image == 0)  # True background pixels in validation\n",
        "    Nbrf = np.sum(val_image == 255)  # True foreground pixels in validation\n",
        "\n",
        "    # Calculate Number of Background Classification Errors (Nbrbe)\n",
        "    Nbrbe = np.sum((seg_image == 255) & (val_image == 0))  # Foreground classified as background\n",
        "\n",
        "    # Calculate Number of Foreground Classification Errors (Nbrfe)\n",
        "    Nbrfe = np.sum((seg_image == 0) & (val_image == 255))  # Background classified as foreground\n",
        "\n",
        "    # Calculate Prob1 and Prob2\n",
        "    Prob1 = Nbrbe / Nbrb if Nbrb > 0 else 0  # Probability that foreground is classified as background\n",
        "    Prob2 = Nbrfe / Nbrf if Nbrf > 0 else 0  # Probability that background is classified as foreground\n",
        "\n",
        "    # Calculate ProbErr\n",
        "    ProbErr = (Prob1 + Prob2) / 2\n",
        "\n",
        "    return Prob1, Prob2, ProbErr\n",
        "\n",
        "# Directories\n",
        "segmentation_dir = '/content/DBs_Dataset/segmented_images'\n",
        "validation_dir = '/content/DBs_Dataset/validation_images'\n",
        "\n",
        "# List of image prefixes for DB1, DB2, DB3, DB4\n",
        "datasets = ['DB1', 'DB2', 'DB3', 'DB4']\n",
        "\n",
        "# Initialize a dictionary to store the results\n",
        "final_results = {}\n",
        "\n",
        "# Calculate the error probabilities for each image in the segmentation folder\n",
        "for dataset in datasets:\n",
        "    # Store the results for the current dataset\n",
        "    Prob1_values = []\n",
        "    Prob2_values = []\n",
        "    ProbErr_values = []\n",
        "\n",
        "    # Loop through each image in the dataset's segmentation folder\n",
        "    for image_path in glob(os.path.join(segmentation_dir, f'FVC2004_{dataset}_*.tif')):\n",
        "        # Construct the corresponding validation image path\n",
        "        val_image_name = os.path.basename(image_path).replace('_paperseg.tif', 'seg.tif')\n",
        "        val_image_path = os.path.join(validation_dir, val_image_name)\n",
        "\n",
        "        # Calculate the error probabilities\n",
        "        Prob1, Prob2, ProbErr = calculate_error_probabilities(image_path, val_image_path)\n",
        "\n",
        "        if Prob1 is not None:\n",
        "            # Store the values in the corresponding lists\n",
        "            Prob1_values.append(Prob1)\n",
        "            Prob2_values.append(Prob2)\n",
        "            ProbErr_values.append(ProbErr)\n",
        "\n",
        "    # Calculate the average for each probability\n",
        "    if Prob1_values:\n",
        "        avg_Prob1 = np.mean(Prob1_values)\n",
        "        avg_Prob2 = np.mean(Prob2_values)\n",
        "        avg_ProbErr = np.mean(ProbErr_values)\n",
        "\n",
        "        # Store the averages for the dataset\n",
        "        final_results[dataset] = {\n",
        "            'avg_Prob1': avg_Prob1,\n",
        "            'avg_Prob2': avg_Prob2,\n",
        "            'avg_ProbErr': avg_ProbErr\n",
        "        }\n",
        "\n",
        "# Print the final average results for each dataset\n",
        "for dataset, metrics in final_results.items():\n",
        "    print(f\"Average Results for {dataset}:\")\n",
        "    print(f\"avg_Prob1: {metrics['avg_Prob1']:.4f}, avg_Prob2: {metrics['avg_Prob2']:.4f}, avg_ProbErr: {metrics['avg_ProbErr']:.4f}\")\n",
        "\n",
        "# Calculate and print overall averages across all datasets\n",
        "all_Prob1 = [metrics['avg_Prob1'] for metrics in final_results.values()]\n",
        "all_Prob2 = [metrics['avg_Prob2'] for metrics in final_results.values()]\n",
        "all_ProbErr = [metrics['avg_ProbErr'] for metrics in final_results.values()]\n",
        "\n",
        "print(\"\\nOverall Average Results:\")\n",
        "print(f\"Overall avg_Prob1: {np.mean(all_Prob1):.4f}, Overall avg_Prob2: {np.mean(all_Prob2):.4f}, Overall avg_ProbErr: {np.mean(all_ProbErr):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj-8qIKoKEXM",
        "outputId": "aaed53c1-5909-4bc6-ae6b-13dd0169462b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Results for DB1:\n",
            "avg_Prob1: 0.0622, avg_Prob2: 0.0080, avg_ProbErr: 0.0351\n",
            "Average Results for DB2:\n",
            "avg_Prob1: 0.2616, avg_Prob2: 0.2997, avg_ProbErr: 0.2806\n",
            "Average Results for DB3:\n",
            "avg_Prob1: 0.0324, avg_Prob2: 0.0660, avg_ProbErr: 0.0492\n",
            "Average Results for DB4:\n",
            "avg_Prob1: 0.0880, avg_Prob2: 0.1139, avg_ProbErr: 0.1010\n",
            "\n",
            "Overall Average Results:\n",
            "Overall avg_Prob1: 0.1110, Overall avg_Prob2: 0.1219, Overall avg_ProbErr: 0.1165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ge-3KiCFKSxw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}